{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<br>", "This is mostly just the tutorial from<br>", "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html<br>", "with the following changes:<br>", " 1. read data from files<br>", " 2. separate test data and validation data<br>", " 3. add tqdm with loss metrics<br>", " 4. early stopping based on validation loss<br>", " 5. track accuracy during training / validation<br>", "\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["pylint: disable=invalid-name,redefined-outer-name"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from typing import Iterable, Mapping, Dict, Tuple, List"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "import tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["torch.manual_seed(1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data(file_path: str) -> Tuple[List[str], List[str]]:\n", "    \"\"\"\n", "    One sentence per line, formatted like\n", "        The###DET dog###NN ate###V the###DET apple###NN\n", "    Returns a list of pairs (tokenized_sentence, tags)\n", "    \"\"\"\n", "    data = []\n", "    with open(file_path) as f:\n", "        for line in f:\n", "            pairs = line.strip().split()\n", "            sentence, tags = zip(*(pair.split(\"###\") for pair in pairs))\n", "            data.append((sentence, tags))\n", "    return data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["training_data = load_data('tutorials/tagger/training.txt')\n", "validation_data = load_data('tutorials/tagger/validation.txt')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prepare_sequence(seq: Iterable[str], to_ix: Mapping[str, int]) -> torch.Tensor:\n", "    idxs = [to_ix[w] for w in seq]\n", "    return torch.tensor(idxs, dtype=torch.long)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["word_to_ix: Dict[str, int] = {}\n", "tag_to_ix: Dict[str, int] = {}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for sent, tags in training_data + validation_data:\n", "    for word in sent:\n", "        if word not in word_to_ix:\n", "            word_to_ix[word] = len(word_to_ix)\n", "    for tag in tags:\n", "        if tag not in tag_to_ix:\n", "            tag_to_ix[tag] = len(tag_to_ix)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["EMBEDDING_DIM = 6\n", "HIDDEN_DIM = 6"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class LSTMTagger(nn.Module):\n", "    def __init__(self,\n", "                 embedding_dim: int,\n", "                 hidden_dim: int,\n", "                 vocab_size: int,\n", "                 tagset_size: int) -> None:\n", "        super(LSTMTagger, self).__init__()\n", "        self.hidden_dim = hidden_dim\n", "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n", "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n", "        # with dimensionality hidden_dim.\n", "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n\n", "        # The linear layer that maps from hidden state space to tag space\n", "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n", "        self.hidden = self.init_hidden()\n", "    def init_hidden(self) -> Tuple[torch.Tensor, torch.Tensor]:\n", "        # Before we've done anything, we dont have any hidden state.\n", "        # Refer to the Pytorch documentation to see exactly\n", "        # why they have this dimensionality.\n", "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n", "        return (torch.zeros(1, 1, self.hidden_dim),\n", "                torch.zeros(1, 1, self.hidden_dim))\n", "    def forward(self, sentence: torch.Tensor) -> torch.Tensor:\n", "        embeds = self.word_embeddings(sentence)\n", "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n", "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n", "        tag_scores = F.log_softmax(tag_space, dim=1)\n", "        return tag_scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n", "loss_function = nn.NLLLoss()\n", "optimizer = optim.SGD(model.parameters(), lr=0.1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["See what the scores are before training<br>", "Note that element i,j of the output is the score for tag j for word i.<br>", "Here we don't need to train, so the code is wrapped in torch.no_grad()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with torch.no_grad():\n", "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n", "    tag_scores = model(inputs)\n", "    print(tag_scores)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["validation_losses = []\n", "patience = 10"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for epoch in range(1000):\n", "    training_loss = 0.0\n", "    validation_loss = 0.0\n", "    for dataset, training in [(training_data, True), (validation_data, False)]:\n", "        correct = total = 0\n", "        torch.set_grad_enabled(training)\n", "        t = tqdm.tqdm(dataset)\n", "        for i, (sentence, tags) in enumerate(t):\n", "            # Step 1. Remember that Pytorch accumulates gradients.\n", "            # We need to clear them out before each instance\n", "            model.zero_grad()\n", "            # Also, we need to clear out the hidden state of the LSTM,\n", "            # detaching it from its history on the last instance.\n", "            model.hidden = model.init_hidden()\n", "            # Step 2. Get our inputs ready for the network, that is, turn them into\n", "            # Tensors of word indices.\n", "            sentence_in = prepare_sequence(sentence, word_to_ix)\n", "            targets = prepare_sequence(tags, tag_to_ix)\n", "            # Step 3. Run our forward pass.\n", "            tag_scores = model(sentence_in)\n", "            # Step 4. Compute the loss, gradients, and update the parameters by\n", "            #  calling optimizer.step()\n", "            loss = loss_function(tag_scores, targets)\n", "            predictions = tag_scores.max(-1)[1]\n", "            correct += (predictions == targets).sum().item()\n", "            total += len(targets)\n", "            accuracy = correct / total\n", "            if training:\n", "                loss.backward()\n", "                training_loss += loss.item()\n", "                t.set_postfix(training_loss=training_loss / (i + 1), accuracy=accuracy)\n", "                optimizer.step()\n", "            else:\n", "                validation_loss += loss.item()\n", "                t.set_postfix(validation_loss=validation_loss / (i + 1), accuracy=accuracy)\n", "    validation_losses.append(validation_loss)\n", "    if (patience and\n", "                len(validation_losses) >= patience and\n", "                validation_losses[-patience] == min(validation_losses[-patience:])):\n", "        print(\"patience reached, stopping early\")\n", "        break"]}, {"cell_type": "markdown", "metadata": {}, "source": ["See what the scores are after training"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with torch.no_grad():\n", "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n", "    tag_scores = model(inputs)\n\n", "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n", "    # for word i. The predicted tag is the maximum scoring tag.\n", "    # Here, we can see the predicted sequence below is\n", "    # DET NN V DET NN, the correct sequence!\n", "    print(tag_scores)\n", "    tag_ids = torch.argmax(tag_scores, dim=-1).tolist()\n", "    print([[\"DET\", \"NN\", \"V\"][i] for i in tag_ids])"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}